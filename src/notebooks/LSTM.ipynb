{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: LSTM sequence model\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local modules from 'src/utils' as package 'utils'\n",
    "import sys; sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trajectory dataset from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_io import read_trajectory_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading .csv files: 1it [00:05,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataframe.\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path(\"../../data/\")\n",
    "train_set, test_set, validation_set, visualization_set = read_trajectory_datasets(data_folder, 0.8, 0.15, 0.045, 0.005, 64, standardize_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape 8 / 3 of total 84044 data rows!\n"
     ]
    }
   ],
   "source": [
    "# FIXME: Total loaded size correct?\n",
    "input_shape, output_shape = 8, 3\n",
    "print(f\"Data shape {input_shape} / {output_shape} of total {len(train_set) + len(test_set) + len(validation_set) + len(visualization_set)} data rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, out_dim: int, dropout_lstm: float = 0.25, dropout_final: float = 0.25,\n",
    "                 num_lstm_layers: int = 1, bidirectional: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.total_epochs = 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.d = 2 if bidirectional else 1\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_lstm_layers, dropout=dropout_lstm, bidirectional=bidirectional)\n",
    "        self.final_dropout = nn.Dropout(dropout_final)\n",
    "        self.out = nn.Linear(hidden_dim * self.d, out_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size = x.shape[1]\n",
    "        # expect x to be of shape (sequence_length, batch_size, input_dim)\n",
    "        h0 = torch.randn(self.d * self.num_lstm_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.randn(self.d * self.num_lstm_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # output shape is (sequence_length, batch_size, d * hidden_dim)\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        output = self.final_dropout(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = DecoderLSTM(input_shape, 6, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameter, functions and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.file_io import save_model\n",
    "from utils.file_io import define_dataloader_from_subset\n",
    "from utils.evaluation import compute_loss_on\n",
    "from utils.loss_functions import maximum_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"../../models/lstm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = model_path / \".env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "learning_rate = float(os.getenv(\"LEARNING_RATE\"))\n",
    "batch_size = int(os.getenv(\"BATCH_SIZE\"))\n",
    "num_epochs = int(os.getenv(\"NUM_EPOCHS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_function = maximum_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader, test_dataloader = define_dataloader_from_subset(train_set, validation_set, test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_dataloader: DataLoader, model, loss_function, optimizer, device: torch.device, report_interval: int = 128):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, (inputs, true_values) in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        true_values = true_values.to(device)\n",
    "        \n",
    "        inputs_shape, true_values_shape = inputs.size(), true_values.size()\n",
    "        inputs = inputs.view(inputs_shape[1], inputs_shape[0], inputs_shape[2])\n",
    "        true_values = true_values.view(true_values_shape[1], true_values_shape[0], true_values_shape[2])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, true_values)\n",
    "        running_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % report_interval == report_interval - 1:\n",
    "            last_loss = running_loss / report_interval\n",
    "            print(f\"batch {i + 1}, Mean Squared Error: {last_loss}\")\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, train_dataloader: DataLoader, validation_dataloader: DataLoader, model: nn.Module,\n",
    "           loss_function, optimizer, checkpoint_path: Path, device: torch.device = 'cpu', report_interval: int = 1000) -> nn.Module:\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(model.total_epochs, epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "\n",
    "        model.train(True)\n",
    "        avg_loss = train_epoch(train_dataloader, model, loss_function, optimizer, device, report_interval)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_val_loss = compute_loss_on(validation_dataloader, model, loss_function, device=device)\n",
    "\n",
    "        print(f\"Loss on train: {avg_loss}, loss on validation: {avg_val_loss}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_path = checkpoint_path / f\"{checkpoint_path.name}_{epoch}.pt\"\n",
    "            save_model(model, model_path)\n",
    "\n",
    "        model.total_epochs += 1\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss on train: 0, loss on validation: 6808.64013671875\n",
      "Epoch: 2\n",
      "Loss on train: 0, loss on validation: 6529.8212890625\n",
      "Epoch: 3\n",
      "Loss on train: 0, loss on validation: 6235.59619140625\n",
      "Epoch: 4\n",
      "Loss on train: 0, loss on validation: 5836.76953125\n",
      "Epoch: 5\n",
      "Loss on train: 0, loss on validation: 5427.1640625\n",
      "Epoch: 6\n",
      "Loss on train: 0, loss on validation: 5049.01513671875\n",
      "Epoch: 7\n",
      "Loss on train: 0, loss on validation: 4673.3544921875\n",
      "Epoch: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m last_model \u001b[39m=\u001b[39m train(num_epochs, train_dataloader, validation_dataloader, model, loss_function, optimizer, model_path, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[1;32m/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, model, loss_function, optimizer, device, report_interval)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, true_values) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pandavid/uni/WiSe2324/ProjectWork/machine-boom-project/src/notebooks/LSTM.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     true_values \u001b[39m=\u001b[39m true_values\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:362\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, indices: List[\u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[T_co]:\n\u001b[1;32m    359\u001b[0m     \u001b[39m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m--> 362\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/mnt/src/notebooks/../utils/trajectory_dataset.py:23\u001b[0m, in \u001b[0;36mTrajectoryDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m start_index \u001b[39m=\u001b[39m index \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_length\n\u001b[1;32m     22\u001b[0m end_index \u001b[39m=\u001b[39m start_index \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_length \u001b[39mif\u001b[39;00m start_index \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_length \u001b[39m<\u001b[39m dataframe_length \u001b[39melse\u001b[39;00m dataframe_length\n\u001b[0;32m---> 23\u001b[0m \u001b[39mslice\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miloc[\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(start_index, end_index))]\n\u001b[1;32m     24\u001b[0m features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mstack(\u001b[39mslice\u001b[39m\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_numpy(), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     25\u001b[0m true_lowpoints \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mstack(\u001b[39mslice\u001b[39m\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto_numpy(), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1705\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# a list of integers\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_list_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1707\u001b[0m \u001b[39m# a single integer\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1709\u001b[0m     key \u001b[39m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1676\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[39mReturn Series values by list or array of integers.\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39m`axis` can only be zero.\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_take_with_is_copy(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1677\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1678\u001b[0m     \u001b[39m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4088\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m, indices, axis: Axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   4079\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4081\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4086\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4087\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4088\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   4089\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4090\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4068\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[39m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4064\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\n\u001b[1;32m   4065\u001b[0m         indices\u001b[39m.\u001b[39mstart, indices\u001b[39m.\u001b[39mstop, indices\u001b[39m.\u001b[39mstep, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp\n\u001b[1;32m   4066\u001b[0m     )\n\u001b[0;32m-> 4068\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[1;32m   4069\u001b[0m     indices,\n\u001b[1;32m   4070\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[1;32m   4071\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   4072\u001b[0m )\n\u001b[1;32m   4073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[39m=\u001b[39mnew_data\u001b[39m.\u001b[39maxes)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   4074\u001b[0m     \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4075\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py:877\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    874\u001b[0m indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[1;32m    876\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 877\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m    878\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[1;32m    879\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[1;32m    880\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    881\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    882\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    883\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py:670\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    663\u001b[0m     new_blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    664\u001b[0m         indexer,\n\u001b[1;32m    665\u001b[0m         fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m    666\u001b[0m         only_slice\u001b[39m=\u001b[39monly_slice,\n\u001b[1;32m    667\u001b[0m         use_na_proxy\u001b[39m=\u001b[39muse_na_proxy,\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m    671\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[1;32m    672\u001b[0m             indexer,\n\u001b[1;32m    673\u001b[0m             axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    674\u001b[0m             fill_value\u001b[39m=\u001b[39m(\n\u001b[1;32m    675\u001b[0m                 fill_value \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m blk\u001b[39m.\u001b[39mfill_value\n\u001b[1;32m    676\u001b[0m             ),\n\u001b[1;32m    677\u001b[0m         )\n\u001b[1;32m    678\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    679\u001b[0m     ]\n\u001b[1;32m    681\u001b[0m new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m    682\u001b[0m new_axes[axis] \u001b[39m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py:671\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    663\u001b[0m     new_blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    664\u001b[0m         indexer,\n\u001b[1;32m    665\u001b[0m         fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m    666\u001b[0m         only_slice\u001b[39m=\u001b[39monly_slice,\n\u001b[1;32m    667\u001b[0m         use_na_proxy\u001b[39m=\u001b[39muse_na_proxy,\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 671\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    672\u001b[0m             indexer,\n\u001b[1;32m    673\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    674\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    675\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    676\u001b[0m             ),\n\u001b[1;32m    677\u001b[0m         )\n\u001b[1;32m    678\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    679\u001b[0m     ]\n\u001b[1;32m    681\u001b[0m new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m    682\u001b[0m new_axes[axis] \u001b[39m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py:1061\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1061\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m   1062\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[1;32m   1063\u001b[0m )\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[39m#  these assertions\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1068\u001b[0m     \u001b[39m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     \u001b[39m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/array_algos/take.py:118\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[1;32m    117\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/array_algos/take.py:163\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m flip_order:\n\u001b[1;32m    166\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_model = train(num_epochs, train_dataloader, validation_dataloader, model, loss_function, optimizer, model_path, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import compute_sliding_window_predictions, compute_losses_from\n",
    "from utils.visualization import create_trace_animation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation on the cpu\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_true = compute_sliding_window_predictions(test_dataloader, last_model, device=device)\n",
    "test_losses = compute_losses_from(y, y_true, loss_function)\n",
    "print(f\"The mean squared error on test is: {test_losses.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw prediction/truth traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    " \n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams['figure.dpi'] = 150  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_dataloader = DataLoader(visualization_set, batch_size=batch_size, shuffle=False)\n",
    "y, y_true = compute_sliding_window_predictions(visualization_dataloader, last_model, device=device)\n",
    "y, y_true = y.numpy(), y_true.numpy()\n",
    "y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "y_true = y_true.reshape(y_true.shape[0] * y_true.shape[1], y_true.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = create_trace_animation(y, y_true)\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_io import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = DecoderLSTM(input_shape, [32, 16], output_shape)\n",
    "model_state_dict = load_model(model_path)\n",
    "loaded_model.load_state_dict(model_state_dict)\n",
    "loaded_model.eval()\n",
    "y, y_true = compute_predictions(test_dataloader, loaded_model, device)\n",
    "test_losses = compute_losses_from(y, y_true, loss_function)\n",
    "print(f\"The mean squared error of the loaded model on test is: {test_losses.mean()}\")\n",
    "animation = create_trace_animation(y.numpy(), y_true.numpy())\n",
    "HTML(animation.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
